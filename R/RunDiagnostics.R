# Copyright 2021 Observational Health Data Sciences and Informatics
#
# This file is part of CohortDiagnostics
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.

#' Run cohort diagnostics
#'
#' @description
#' Runs cohort diagnostics on cohorts specified in cohortToCreateFile (file) or cohortSetReference. The function
#' checks if the specified cohorts are instantiated, and only runs diagnostics on the instantiated
#' cohorts (i.e. > 0 rows in cohort table).
#'
#' Characterization:
#' If runCohortCharacterization argument is TRUE, then \code{RFeatureExtraction::createTemporalCovariateSettings}
#' is used as default.
#'
#' @template Connection
#'
#' @template CdmDatabaseSchema
#' @template VocabularyDatabaseSchema
#' @template CohortDatabaseSchema
#' @template TempEmulationSchema
#' @template OracleTempSchema
#'
#' @template CohortTable
#'
#' @template CohortSetSpecs
#'
#' @template CohortSetReference
#' @param inclusionStatisticsFolder   The folder where the inclusion rule statistics are stored. Can be
#'                                    left NULL if \code{runInclusionStatistics = FALSE}.
#' @param exportFolder                The folder where the output will be exported to. If this folder
#'                                    does not exist it will be created.
#' @param cohortIds                   Optionally, provide a subset of cohort IDs to restrict the
#'                                    diagnostics to.
#' @param databaseId                  A short string for identifying the database (e.g. 'Synpuf').
#' @param databaseName                The full name of the database. If NULL, defaults to databaseId.
#' @param databaseDescription         A short description (several sentences) of the database. If NULL, defaults to databaseId.
#' @template cdmVersion
#' @param runInclusionStatistics      Generate and export statistic on the cohort inclusion rules?
#' @param runConceptSetDiagnostics    Concept Set Diagnostics includes concept counts, concepts in data source,
#'                                    index event breakdown, concept cooccurrence, excluded concepts,
#'                                    resolved concepts. This function call now supersedes runIncludedSourceConcepts,
#'                                    runOrphanConcepts, runBreakdownIndexEvents.
#' @template IndexDateDiagnosticsRelativeDays
#' @param runIncludedSourceConcepts   (Deprecated) Generate and export the source concepts included in the cohorts?
#' @param runOrphanConcepts           (Deprecated) Generate and export potential orphan concepts?
#' @param runVisitContext             Generate and export index-date visit context?
#' @param runBreakdownIndexEvents     (Deprecated) Generate and export the breakdown of index events?
#' @param runIncidenceRate            Generate and export the cohort incidence  rates?
#' @param runCohortTimeSeries         Generate and export the cohort level time series?
#' @param runDataSourceTimeSeries     Generate and export the Data source level time series? i.e.
#'                                    using all persons found in observation period table.
#' @param runCohortRelationship       Do you want to compute temporal relationship between the cohorts being diagnosed. This
#'                                    diagnostics is needed for cohort as feature characterization.
#' @param runCohortCharacterization   Generate and export the temporal cohort characterization?
#'                                    Only records with values greater than 0.001 are returned.
#'                                    We use FeatureExtraction createTemporalCovariateSettings function to specificy
#'                                    covariate settings - because the results are computed over multiple time windows.
#' @param temporalCovariateSettings   Either an object of type \code{covariateSettings} as created using one of
#'                                    the createTemporalCovariateSettings function in the FeatureExtraction package, or a list
#'                                    of such objects.
#' @param minCellCount                The minimum cell count for fields contains person counts or fractions.
#' @param incremental                 Create only cohort diagnostics that haven't been created before?
#' @param incrementalFolder           If \code{incremental = TRUE}, specify a folder where records are kept
#'                                    of which cohort diagnostics has been executed.
#' @export
runCohortDiagnostics <- function(packageName = NULL,
                                 cohortToCreateFile = "settings/CohortsToCreate.csv",
                                 baseUrl = NULL,
                                 cohortSetReference = NULL,
                                 connectionDetails = NULL,
                                 connection = NULL,
                                 cdmDatabaseSchema,
                                 oracleTempSchema = NULL,
                                 tempEmulationSchema = NULL,
                                 cohortDatabaseSchema,
                                 vocabularyDatabaseSchema = cdmDatabaseSchema,
                                 cohortTable = "cohort",
                                 cohortIds = NULL,
                                 inclusionStatisticsFolder = file.path(exportFolder, "inclusionStatistics"),
                                 exportFolder,
                                 databaseId,
                                 databaseName = databaseId,
                                 databaseDescription = databaseId,
                                 cdmVersion = 5,
                                 runInclusionStatistics = TRUE,
                                 runConceptSetDiagnostics = TRUE,
                                 indexDateDiagnosticsRelativeDays = c(-5:5),  #part of index event breakdown
                                 runIncludedSourceConcepts = FALSE, #deprecated
                                 runOrphanConcepts = FALSE, #deprecated
                                 runVisitContext = TRUE,
                                 runBreakdownIndexEvents = FALSE, #deprecated
                                 runIncidenceRate = TRUE,
                                 runCohortTimeSeries = TRUE,
                                 runDataSourceTimeSeries = TRUE,
                                 runCohortRelationship = TRUE,
                                 runCohortCharacterization = TRUE,
                                 temporalCovariateSettings = FeatureExtraction::createTemporalCovariateSettings(
                                   useDemographicsGender = TRUE,
                                   useDemographicsAge = TRUE,
                                   useDemographicsAgeGroup = TRUE,
                                   useDemographicsRace = TRUE,
                                   useDemographicsEthnicity = TRUE,
                                   useDemographicsIndexYear = TRUE,
                                   useDemographicsIndexMonth = TRUE,
                                   useDemographicsIndexYearMonth = TRUE,
                                   useDemographicsPriorObservationTime = TRUE,
                                   useDemographicsPostObservationTime = TRUE,
                                   useDemographicsTimeInCohort = TRUE,
                                   useConditionOccurrence = TRUE,
                                   useProcedureOccurrence = TRUE,
                                   useDrugEraStart = TRUE,
                                   useMeasurement = TRUE,
                                   useConditionEraStart = TRUE,
                                   useConditionEraOverlap = TRUE,
                                   useConditionEraGroupStart = FALSE,
                                   useConditionEraGroupOverlap = TRUE,
                                   useDrugExposure = FALSE, #leads to too many concept id
                                   useDrugEraOverlap = FALSE,
                                   useDrugEraGroupStart = FALSE,
                                   useDrugEraGroupOverlap = TRUE,
                                   useObservation = TRUE,
                                   useDeviceExposure = TRUE,
                                   useCharlsonIndex = TRUE,
                                   useDcsi = TRUE,
                                   useChads2 = TRUE,
                                   useChads2Vasc = TRUE,
                                   useHfrs = FALSE,
                                   temporalStartDays = c(
                                     -365,
                                     -30,
                                     0,
                                     1,
                                     31,
                                     -9999,
                                     -365,
                                     -180,
                                     -30,
                                     -9999,
                                     -365,
                                     -180,
                                     -30,
                                     -9999,
                                     seq(from = -421, to = -31, by = 30),
                                     seq(from = 0, to = 390, by = 30),
                                     seq(from = -5, to = 5, by = 1)
                                   ),
                                   temporalEndDays = c(
                                     -31,
                                     -1,
                                     0,
                                     30,
                                     365,
                                     0,
                                     0,
                                     0,
                                     0,
                                     -1,
                                     -1,
                                     -1,
                                     -1,
                                     9999,
                                     seq(from = -391, to = -1, by = 30),
                                     seq(from = 30, to = 420, by = 30),
                                     seq(from = -5, to = 5, by = 1)
                                   )
                                 ),
                                 minCellCount = 5,
                                 incremental = TRUE,
                                 incrementalFolder = file.path(exportFolder, "incremental")) {
  startDateTime <- Sys.time()
  if (all(is.null(connectionDetails),
          is.null(connection))) {
    stop('Please provide either connection or connectionDetails to connect to database.')
  }
  # Set up connection to server----
  if (is.null(connection)) {
    if (!is.null(connectionDetails)) {
      connection <- DatabaseConnector::connect(connectionDetails)
      on.exit(DatabaseConnector::disconnect(connection))
    }
  }
  tables <-
    DatabaseConnector::getTableNames(connection, cohortDatabaseSchema)
  if (!toupper(cohortTable) %in% toupper(tables)) {
    stop(
      "Cannot find cohort table. Did you instantiate cohorts? \n Try running function instantiateCohortSet with option createCohortTable = TRUE, or \n use function Try functions createCohortTable if you only want to create the cohort table without instantiating cohorts."
    )
  }
  
  if (runIncludedSourceConcepts) {
    warning(
      "runIncludedSourceConcepts is deprecated. Running runConceptSetDiagnostics instead."
    )
    runConceptSetDiagnostics <- TRUE
  }
  if (runOrphanConcepts) {
    warning("runOrphanConcepts is deprecated. Running runConceptSetDiagnostics instead.")
    runConceptSetDiagnostics <- TRUE
  }
  if (runBreakdownIndexEvents) {
    warning(
      "runBreakdownIndexEvents is deprecated. Running runConceptSetDiagnostics instead."
    )
    runConceptSetDiagnostics <- TRUE
  }
  
  ParallelLogger::logInfo("Run Cohort Diagnostics started at ", startDateTime, '. Initiating...')
  ParallelLogger::logInfo("")
  ParallelLogger::logInfo(" - Database id: ", databaseId)
  ParallelLogger::logInfo(" - Database Name: ", databaseName)
  ParallelLogger::logInfo(" - Database Description: ", databaseDescription)
  ParallelLogger::logInfo("")
  ParallelLogger::logInfo(" - Cohort table: ", cohortTable)
  
  # collect arguments that were passed to cohort diagnostics at initiation
  argumentsAtDiagnosticsInitiation <- formals(runCohortDiagnostics)
  argumentsAtDiagnosticsInitiationJson <-
    list(
      runInclusionStatistics = argumentsAtDiagnosticsInitiation$runInclusionStatistics,
      runConceptSetDiagnostics = argumentsAtDiagnosticsInitiation$runConceptSetDiagnostics,
      indexDateDiagnosticsRelativeDays = argumentsAtDiagnosticsInitiation$indexDateDiagnosticsRelativeDays,
      runVisitContext = argumentsAtDiagnosticsInitiation$runVisitContext,
      runIncidenceRate = argumentsAtDiagnosticsInitiation$runIncidenceRate,
      runCohortTimeSeries = argumentsAtDiagnosticsInitiation$runCohortTimeSeries,
      runDataSourceTimeSeries = argumentsAtDiagnosticsInitiation$runDataSourceTimeSeries,
      runCohortRelationship = argumentsAtDiagnosticsInitiation$runCohortRelationship,
      runCohortCharacterization = argumentsAtDiagnosticsInitiation$runCohortCharacterization,
      minCellCount = argumentsAtDiagnosticsInitiation$minCellCount,
      incremental = argumentsAtDiagnosticsInitiation$incremental,
      covariateSettings = argumentsAtDiagnosticsInitiation$covariateSettings,
      temporalCovariateSettings = argumentsAtDiagnosticsInitiation$temporalCovariateSettings
    ) %>%
    RJSONIO::toJSON(digits = 23, pretty = TRUE)
  
  # take package dependency snapshot
  packageDependencySnapShotJson <-
    takepackageDependencySnapshot() %>%
    RJSONIO::toJSON(digits = 23, pretty = TRUE)
  
  # Execution mode determination----
  if (!is.null(cohortSetReference)) {
    ParallelLogger::logInfo(" - Found cohortSetReference. Cohort Diagnostics will run in WebApi mode.")
    cohortToCreateFile <- NULL
  }
  
  if (all(!is.null(oracleTempSchema), is.null(tempEmulationSchema))) {
    tempEmulationSchema <- oracleTempSchema
    warning(
      ' - OracleTempSchema has been deprecated by DatabaseConnector. Please use tempEmulationSchema instead.'
    )
  }
  databaseId <- as.character(databaseId)
  if (any(is.null(databaseName), is.na(databaseName))) {
    databaseName <- databaseId
    ParallelLogger::logTrace(' - Databasename was not provided.')
  }
  if (any(is.null(databaseDescription), is.na(databaseDescription))) {
    databaseDescription <- databaseId
    ParallelLogger::logTrace(' - Databasedescription was not provided.')
  }
  
  # Assert checks----
  errorMessage <- checkmate::makeAssertCollection()
  checkmate::assertLogical(runInclusionStatistics, add = errorMessage)
  checkmate::assertLogical(runConceptSetDiagnostics, add = errorMessage)
  checkmate::assertLogical(runIncidenceRate, add = errorMessage)
  checkmate::assertInt(
    x = cdmVersion,
    na.ok = FALSE,
    lower = 5,
    upper = 5,
    null.ok = FALSE,
    add = errorMessage
  )
  minCellCount <- utils::type.convert(minCellCount, as.is = TRUE)
  checkmate::assertInteger(x = minCellCount, lower = 0, add = errorMessage)
  checkmate::assertLogical(incremental, add = errorMessage)
  
  if (any(
    runInclusionStatistics,
    runConceptSetDiagnostics,
    runIncidenceRate,
    runCohortTimeSeries,
    runCohortRelationship,
    runCohortCharacterization
  )) {
    checkmate::assertCharacter(x = cdmDatabaseSchema,
                               min.len = 1,
                               add = errorMessage)
    checkmate::assertCharacter(x = vocabularyDatabaseSchema,
                               min.len = 1,
                               add = errorMessage)
    checkmate::assertCharacter(x = cohortDatabaseSchema,
                               min.len = 1,
                               add = errorMessage)
    checkmate::assertCharacter(x = cohortTable,
                               min.len = 1,
                               add = errorMessage)
    checkmate::assertCharacter(x = databaseId,
                               min.len = 1,
                               add = errorMessage)
  }
  checkmate::reportAssertions(collection = errorMessage)
  
  errorMessage <-
    createIfNotExist(type = "folder",
                     name = exportFolder,
                     errorMessage = errorMessage)
  
  if (incremental) {
    errorMessage <-
      createIfNotExist(type = "folder",
                       name = incrementalFolder,
                       errorMessage = errorMessage)
  }
  if (isTRUE(runInclusionStatistics)) {
    errorMessage <-
      createIfNotExist(type = "folder",
                       name = inclusionStatisticsFolder,
                       errorMessage = errorMessage)
  }
  checkmate::reportAssertions(collection = errorMessage)
  
  # Cohort Info checks----
  cohorts <- getCohortsJsonAndSql(
    # get cohort json/sql from webapi or package
    packageName = packageName,
    cohortToCreateFile = cohortToCreateFile,
    baseUrl = baseUrl,
    cohortSetReference = cohortSetReference,
    cohortIds = cohortIds
  )
  
  if (any(is.null(cohorts),
          nrow(cohorts) == 0)) {
    stop("No cohorts specified, or no matching cohorts found. Aborting.")
  }
  if ('name' %in% colnames(cohorts)) {
    cohorts <- cohorts %>%
      dplyr::select(-.data$name)
  }
  cohortTableColumnNamesObserved <- colnames(cohorts) %>%
    sort()
  cohortTableColumnNamesExpected <-
    getResultsDataModelSpecifications(packageName = 'CohortDiagnostics') %>%
    dplyr::filter(.data$tableName == 'cohort') %>%
    dplyr::pull(.data$fieldName) %>%
    SqlRender::snakeCaseToCamelCase() %>%
    sort()
  cohortTableColumnNamesRequired <-
    getResultsDataModelSpecifications(packageName = 'CohortDiagnostics') %>%
    dplyr::filter(.data$tableName == 'cohort') %>%
    dplyr::filter(.data$isRequired == 'Yes') %>%
    dplyr::pull(.data$fieldName) %>%
    SqlRender::snakeCaseToCamelCase() %>%
    sort()
  
  requiredButNotObsevered <-
    setdiff(x = cohortTableColumnNamesRequired, y = cohortTableColumnNamesObserved)
  if (length(requiredButNotObsevered) > 0) {
    stop(paste(
      "- The following required fields not found in cohort table:",
      paste0(" '", requiredButNotObsevered, "'", collapse = ", ")
    ))
  }
  
  obseveredButNotExpected <-
    setdiff(x = cohortTableColumnNamesObserved,
            y = cohortTableColumnNamesExpected)
  if (length(obseveredButNotExpected) > 0) {
    ParallelLogger::logTrace(
      paste0(
        " - The following columns were found in cohort table, but are not expected - they will be removed:",
        paste0(" '", obseveredButNotExpected, "'", collapse = ",")
      )
    )
  }
  
  for (i in (1:length(cohortTableColumnNamesExpected))) {
    if (!cohortTableColumnNamesExpected[[i]] %in% colnames(cohorts)) {
      cohorts[[cohortTableColumnNamesExpected[[i]]]] <- NA
    }
  }
  
  cohorts <- cohorts %>%
    dplyr::select(cohortTableColumnNamesExpected)
  writeToCsv(data = cohorts,
             fileName = file.path(exportFolder, "cohort.csv"))
  
  # Metadata----
  startMetaData <- Sys.time()
  ## CDM source information----
  cdmSourceInformation <-
    getCdmDataSourceInformation(connection = connection,
                                cdmDatabaseSchema = cdmDatabaseSchema)
  
  ## Vocabulary information----
  vocabularyVersion <-
    renderTranslateQuerySql(
      connection = connection,
      sql = "select * from @vocabulary_database_schema.vocabulary where vocabulary_id = 'None';",
      vocabulary_database_schema = vocabularyDatabaseSchema,
      snakeCaseToCamelCase = TRUE
    ) %>%
    dplyr::pull(.data$vocabularyVersion) %>%
    unique()
  
  ## Observation period----
  ParallelLogger::logTrace(" - Collecting date range from Observational period table.")
  observationPeriodDateRange <- renderTranslateQuerySql(
    connection = connection,
    sql = "SELECT MIN(observation_period_start_date) observation_period_min_date,
             MAX(observation_period_end_date) observation_period_max_date,
             COUNT(distinct person_id) persons,
             COUNT(person_id) records,
             SUM(DATEDIFF(dd, observation_period_start_date, observation_period_end_date)) person_days
             FROM @cdm_database_schema.observation_period;",
    cdm_database_schema = cdmDatabaseSchema,
    snakeCaseToCamelCase = TRUE,
    tempEmulationSchema = tempEmulationSchema
  )
  
  database <- dplyr::tibble(
    databaseId = databaseId,
    databaseName = dplyr::coalesce(databaseName, databaseId),
    description = dplyr::coalesce(databaseDescription, databaseId),
    vocabularyVersionCdm = cdmSourceInformation$vocabularyVersion,
    vocabularyVersion = !!vocabularyVersion,
    isMetaAnalysis = 0
  )
  database <- .replaceNaInDataFrameWithEmptyString(database)
  writeToCsv(data = database,
             fileName = file.path(exportFolder, "database.csv"))
  delta <- Sys.time() - startMetaData
  ParallelLogger::logTrace(paste(
    " - Saving database metadata took",
    signif(delta, 3),
    attr(delta, "units")
  ))
  
  # Incremental mode----
  if (incremental) {
    ParallelLogger::logInfo(" - Working in incremental mode.")
    cohorts$checksum <- computeChecksum(cohorts$sql)
    recordKeepingFile <-
      file.path(incrementalFolder, "CreatedDiagnostics.csv")
    if (file.exists(path = recordKeepingFile)) {
      ParallelLogger::logTrace(
        "  - Found existing record keeping file in incremental folder - CreatedDiagnostics.csv."
      )
    } else {
      ParallelLogger::logTrace(
        "  - Did not find existing record keeping file in incremental folder. Creating file CreatedDiagnostics.csv."
      )
    }
  }
  
  # Counting cohorts----
  ParallelLogger::logTrace(" - Counting records & subjects for instantiated cohorts.")
  output <- getCohortCounts(
    connection = connection,
    cohortDatabaseSchema = cohortDatabaseSchema,
    cohortTable = cohortTable,
    cohortIds = cohorts$cohortId
  ) # cohortCounts is reused
  cohortCounts <- output$cohortCount %>% dplyr::collect()
  # Get instantiated cohorts ----
  instantiatedCohorts <-
    as.double(c(-1)) # set by default to non instantiated
  if (!is.null(cohortCounts)) {
    if (cohortCounts %>% dplyr::summarise(n = dplyr::n()) %>% dplyr::pull(.data$n) > 0) {
      writeToAllOutputToCsv(
        object = output,
        exportFolder = exportFolder,
        databaseId = databaseId,
        incremental = incremental,
        minCellCount = minCellCount
      )
      instantiatedCohorts <- output$cohortCount %>%
        dplyr::pull(.data$cohortId)
      if (length(instantiatedCohorts) < nrow(cohorts)) {
        ParallelLogger::logInfo(
          paste0(
            " - Skipping diagnostics on following cohorts as they were either not instantiated or had no subjects: ",
            paste0(
              setdiff(cohorts$cohortId, instantiatedCohorts),
              collapse = ", "
            )
          )
        )
      }
      Andromeda::close(output)
      rm("output")
    } else {
      warning(
        " - All cohorts were either not instantiated or all have 0 records. All diagnostics will be empty."
      )
    }
  }
  # Inclusion statistics----
  ParallelLogger::logInfo(" - Retrieving inclusion rules from file.")
  if (runInclusionStatistics) {
    startInclusionStatistics <- Sys.time()
    if (any(is.null(instantiatedCohorts),-1 %in% instantiatedCohorts)) {
      ParallelLogger::logTrace("  - Skipping inclusion statistics from files because no cohorts were instantiated.")
    } else {
      subset <- subsetToRequiredCohorts(
        cohorts = cohorts %>%
          dplyr::filter(.data$cohortId %in% instantiatedCohorts),
        task = "runInclusionStatistics",
        incremental = incremental,
        recordKeepingFile = recordKeepingFile
      )
      if (nrow(subset) > 0) {
        if (incremental &&
            (length(instantiatedCohorts) - nrow(subset)) > 0) {
          ParallelLogger::logInfo(sprintf(
            "  - Skipping %s cohorts in incremental mode.",
            length(instantiatedCohorts) - nrow(subset)
          ))
        }
        output <-
          getInclusionStatisticsFromFiles(cohortIds = subset$cohortId,
                                          folder = inclusionStatisticsFolder)
        if (!is.null(output)) {
          writeToAllOutputToCsv(
            object = output,
            exportFolder = exportFolder,
            databaseId = databaseId,
            incremental = incremental,
            minCellCount = minCellCount
          )
          recordTasksDone(
            cohortId = subset$cohortId,
            task = "runInclusionStatistics",
            checksum = subset$checksum,
            recordKeepingFile = recordKeepingFile,
            incremental = incremental
          )
          Andromeda::close(output)
          rm("output")
        } else {
          ParallelLogger::logInfo("  - None of the cohorts had inclusion rules.")
        }
      } else {
        ParallelLogger::logInfo("  - Skipping in incremental mode.")
      }
    }
    delta <- Sys.time() - startInclusionStatistics
    ParallelLogger::logTrace(" - Running Inclusion Statistics took ",
                             signif(delta, 3),
                             " ",
                             attr(delta, "units"))
  }
  
  # Concept set diagnostics----
  if (runConceptSetDiagnostics) {
    ParallelLogger::logInfo(" - Beginning concept set diagnostics.")
    startConceptSetDiagnostics <- Sys.time()
    subset <- subsetToRequiredCohorts(
      cohorts = cohorts,
      task = "runConceptSetDiagnostics",
      incremental = incremental,
      recordKeepingFile = recordKeepingFile
    )
    if (nrow(subset) > 0) {
      if (nrow(cohorts) - nrow(subset) > 0) {
        ParallelLogger::logInfo(sprintf(
          "  - Skipping %s cohorts in incremental mode.",
          nrow(cohorts) - nrow(subset)
        ))
      }
      output <- runConceptSetDiagnostics(
        connection = connection,
        tempEmulationSchema = tempEmulationSchema,
        cdmDatabaseSchema = cdmDatabaseSchema,
        vocabularyDatabaseSchema = vocabularyDatabaseSchema,
        cohorts = cohorts,
        cohortIds = subset$cohortId,
        cohortDatabaseSchema = cohortDatabaseSchema,
        indexDateDiagnosticsRelativeDays = indexDateDiagnosticsRelativeDays,
        cohortTable = cohortTable,
        minCellCount = minCellCount
      )
      writeToAllOutputToCsv(
        object = output,
        exportFolder = exportFolder,
        databaseId = databaseId,
        incremental = incremental,
        minCellCount = minCellCount
      )
      Andromeda::close(output)
      rm("output")
      recordTasksDone(
        cohortId = subset$cohortId,
        task = "runConceptSetDiagnostics",
        checksum = subset$checksum,
        recordKeepingFile = recordKeepingFile,
        incremental = incremental
      )
    } else {
      ParallelLogger::logInfo("  - Skipping in incremental mode.")
    }
    delta <- Sys.time() - startConceptSetDiagnostics
    ParallelLogger::logInfo(
      " - Running Concept Set Diagnostics and saving files took ",
      signif(delta, 3),
      " ",
      attr(delta, "units")
    )
  }
  
  # Visit context----
  if (runVisitContext) {
    ParallelLogger::logInfo("Retrieving visit context for index dates")
    startVisitContext <- Sys.time()
    subset <- subsetToRequiredCohorts(
      cohorts = cohorts %>%
        dplyr::filter(.data$cohortId %in% instantiatedCohorts),
      task = "runVisitContext",
      incremental = incremental,
      recordKeepingFile = recordKeepingFile
    )
    if (nrow(subset) > 0) {
      if (incremental &&
          (length(instantiatedCohorts) - nrow(subset)) > 0) {
        ParallelLogger::logInfo(sprintf(
          " - Skipping %s cohorts in incremental mode.",
          length(instantiatedCohorts) - nrow(subset)
        ))
      }
      output <- runVisitContextDiagnostics(
        connection = connection,
        tempEmulationSchema = tempEmulationSchema,
        cdmDatabaseSchema = cdmDatabaseSchema,
        cohortDatabaseSchema = cohortDatabaseSchema,
        vocabularyDatabaseSchema = vocabularyDatabaseSchema,
        cohortTable = cohortTable,
        cdmVersion = cdmVersion,
        cohortIds = subset$cohortId
      )
      writeToAllOutputToCsv(
        object = output,
        exportFolder = exportFolder,
        databaseId = databaseId,
        incremental = incremental,
        minCellCount = minCellCount
      )
      Andromeda::close(output)
      rm("output")
      recordTasksDone(
        cohortId = subset$cohortId,
        task = "runVisitContext",
        checksum = subset$checksum,
        recordKeepingFile = recordKeepingFile,
        incremental = incremental
      )
    } else {
      ParallelLogger::logInfo("  - Skipping in incremental mode.")
    }
    delta <- Sys.time() - startVisitContext
    ParallelLogger::logInfo(
      " - Running Visit Context and saving files took ",
      signif(delta, 3),
      " ",
      attr(delta, "units")
    )
  }
  
  # Incidence rates----
  if (runIncidenceRate) {
    ParallelLogger::logInfo("Computing incidence rates")
    startIncidenceRate <- Sys.time()
    subset <- subsetToRequiredCohorts(
      cohorts = cohorts %>%
        dplyr::filter(.data$cohortId %in% instantiatedCohorts),
      task = "runIncidenceRate",
      incremental = incremental,
      recordKeepingFile = recordKeepingFile
    )
    if (nrow(subset) > 0) {
      if (incremental &&
          (length(instantiatedCohorts) - nrow(subset)) > 0) {
        ParallelLogger::logInfo(sprintf(
          " - Skipping %s cohorts in incremental mode.",
          length(instantiatedCohorts) - nrow(subset)
        ))
      }
      #incidence rate does not follow the pattern used by other diagnostics
      # in this package because we plan to replace it with the new incidence
      # rate package that will offer better ways to calculate incidence rate
      runIncidenceRate <- function(row) {
        ParallelLogger::logInfo(" - '",
                                row$cohortName,
                                "'")
        cohortExpression <- RJSONIO::fromJSON(row$json, digits = 23)
        washoutPeriod <- tryCatch({
          cohortExpression$PrimaryCriteria$ObservationWindow$PriorDays
        }, error = function(e) {
          0
        })
        data <- runIncidenceRateDiagnostics(
          connection = connection,
          cdmDatabaseSchema = cdmDatabaseSchema,
          tempEmulationSchema = tempEmulationSchema,
          cohortDatabaseSchema = cohortDatabaseSchema,
          cohortTable = cohortTable,
          cohortId = row$cohortId,
          firstOccurrenceOnly = TRUE,
          washoutPeriod = washoutPeriod
        )
        return(data)
      }
      data <-
        lapply(split(subset, subset$cohortId), runIncidenceRate)
      output <- list()
      output$incidenceRate <- dplyr::bind_rows(data)
      data <- NULL
      writeToAllOutputToCsv(
        object = output,
        exportFolder = exportFolder,
        databaseId = databaseId,
        incremental = incremental,
        minCellCount = minCellCount
      )
      # Andromeda::close(output) - not using Andromeda for incidence rate as we plan to replace it with incidence rate package in future
      rm("output")
      recordTasksDone(
        cohortId = subset$cohortId,
        task = "runIncidenceRate",
        checksum = subset$checksum,
        recordKeepingFile = recordKeepingFile,
        incremental = incremental
      )
    } else {
      ParallelLogger::logInfo("  - Skipping in incremental mode.")
    }
    delta <- Sys.time() - startIncidenceRate
    ParallelLogger::logInfo(" - Running Incidence Rate took ",
                            signif(delta, 3),
                            " ",
                            attr(delta, "units"))
  }
  
  # Time Series----
  if (any(runCohortTimeSeries, runDataSourceTimeSeries)) {
    ParallelLogger::logInfo("Computing Time Series")
    startTimeSeries <- Sys.time()
    cohortIds <- NULL
    if (runCohortTimeSeries) {
      subset <- subsetToRequiredCohorts(
        cohorts = cohorts %>%
          dplyr::filter(.data$cohortId %in% instantiatedCohorts),
        task = "runCohortTimeSeries",
        incremental = incremental,
        recordKeepingFile = recordKeepingFile
      )
      cohortIds <- subset$cohortId
      if (nrow(subset) > 0) {
        if (incremental &&
            (length(instantiatedCohorts) - nrow(subset)) > 0) {
          ParallelLogger::logInfo(sprintf(
            " - Skipping %s cohorts in incremental mode.",
            length(instantiatedCohorts) - nrow(subset)
          ))
        }
        output <-
          runCohortTimeSeriesDiagnostics(
            connection = connection,
            tempEmulationSchema = tempEmulationSchema,
            cohortDatabaseSchema = cohortDatabaseSchema,
            cdmDatabaseSchema = cdmDatabaseSchema,
            cohortTable = cohortTable,
            runDataSourceTimeSeries = runDataSourceTimeSeries,
            runCohortTimeSeries = runCohortTimeSeries,
            timeSeriesMinDate = observationPeriodDateRange$observationPeriodMinDate,
            timeSeriesMaxDate = observationPeriodDateRange$observationPeriodMaxDate,
            cohortIds = cohortIds
          )
        writeToAllOutputToCsv(
          object = output,
          exportFolder = exportFolder,
          databaseId = databaseId,
          incremental = incremental,
          minCellCount = minCellCount
        )
        Andromeda::close(output)
        rm("output")
        recordTasksDone(
          cohortId = subset$cohortId,
          task = "runCohortTimeSeries",
          checksum = subset$checksum,
          recordKeepingFile = recordKeepingFile,
          incremental = incremental
        )
      } else {
        ParallelLogger::logInfo("  - Skipping in incremental mode.")
      }
    }
    delta <- Sys.time() - startTimeSeries
    ParallelLogger::logInfo(" - Computing time series took ",
                            signif(delta, 3),
                            " ",
                            attr(delta, "units"))
  }
  
  # Cohort Relationship ----
  if (runCohortRelationship) {
    ParallelLogger::logInfo("Computing Cohort Relationship")
    startCohortRelationship <- Sys.time()
    subset <- subsetToRequiredCohorts(
      cohorts = cohorts %>%
        dplyr::filter(.data$cohortId %in% instantiatedCohorts),
      task = "runCohortRelationship",
      incremental = incremental,
      recordKeepingFile = recordKeepingFile
    )
    if (nrow(subset) > 0) {
      if (incremental &&
          (length(instantiatedCohorts) - nrow(subset)) > 0) {
        ParallelLogger::logInfo(sprintf(
          " - Skipping %s cohort combinations in incremental mode.",
          nrow(cohorts) - nrow(subset)
        ))
      }
      ParallelLogger::logTrace(" - Beginning Cohort Relationship SQL")
      if (all(exists("temporalCovariateSettings"),
              !is.null(temporalCovariateSettings))) {
        temporalStartDays <- temporalCovariateSettings$temporalStartDays
        temporalEndDays <- temporalCovariateSettings$temporalEndDays
      } else {
        temporalStartDays = c(
          -365,
          -30,
          0,
          1,
          31,
          -9999,
          -365,
          -180,
          -30,
          -9999,
          -365,
          -180,
          -30,
          -9999,
          seq(from = -421, to = -31, by = 30),
          seq(from = 0, to = 390, by = 30),
          seq(from = -5, to = 5, by = 1)
        )
        temporalEndDays = c(
          -31,
          -1,
          0,
          30,
          365,
          0,
          0,
          0,
          0,
          -1,
          -1,
          -1,
          -1,
          9999,
          seq(from = -391, to = -1, by = 30),
          seq(from = 30, to = 420, by = 30),
          seq(from = -5, to = 5, by = 1)
        )
      }
      output <-
        runCohortRelationshipDiagnostics(
          connection = connection,
          cohortDatabaseSchema = cohortDatabaseSchema,
          cdmDatabaseSchema = cdmDatabaseSchema,
          tempEmulationSchema = tempEmulationSchema,
          cohortTable = cohortTable,
          targetCohortIds = subset$cohortId,
          comparatorCohortIds = cohorts$cohortId,
          relationshipDays = dplyr::tibble(startDay = temporalStartDays,
                                           endDay = temporalEndDays),
          incremental = incremental,
          incrementalFolder = incrementalFolder
        )
      writeToAllOutputToCsv(
        object = output,
        exportFolder = exportFolder,
        incremental = incremental,
        minCellCount = minCellCount,
        databaseId = databaseId
      )
      Andromeda::close(output)
      rm("output")
      recordTasksDone(
        cohortId = subset$cohortId,
        task = "runCohortRelationship",
        checksum = subset$checksum,
        recordKeepingFile = recordKeepingFile,
        incremental = incremental
      )
    } else {
      ParallelLogger::logInfo("  - Skipping in incremental mode.")
    }
    delta <- Sys.time() - startCohortRelationship
    ParallelLogger::logInfo(" - Computing cohort relationships took ",
                            signif(delta, 3),
                            " ",
                            attr(delta, "units"))
  }
  
  ## Temporal Cohort characterization----
  if (runCohortCharacterization) {
    ParallelLogger::logInfo("Temporal Cohort characterization")
    startTemporalCohortCharacterization <- Sys.time()
    subset <- subsetToRequiredCohorts(
      cohorts = cohorts %>%
        dplyr::filter(.data$cohortId %in% instantiatedCohorts),
      task = "runCohortCharacterization",
      incremental = incremental,
      recordKeepingFile = recordKeepingFile
    )
    if (nrow(subset) > 0) {
      if (incremental &&
          (length(instantiatedCohorts) - nrow(subset)) > 0) {
        ParallelLogger::logInfo(sprintf(
          " - Skipping %s cohorts in incremental mode.",
          length(instantiatedCohorts) - nrow(subset)
        ))
      }
      output <-
        runCohortCharacterizationDiagnostics(
          connection = connection,
          cdmDatabaseSchema = cdmDatabaseSchema,
          tempEmulationSchema = tempEmulationSchema,
          cohortDatabaseSchema = cohortDatabaseSchema,
          cohortTable = cohortTable,
          cohortIds = subset$cohortId,
          covariateSettings = temporalCovariateSettings,
          cdmVersion = cdmVersion
        )
      exportFeatureExtractionOutput(
        featureExtractionDbCovariateData = output,
        databaseId = databaseId,
        incremental = incremental,
        covariateValueFileName = file.path(exportFolder, "covariate_value.csv"),
        covariateValueContFileName = file.path(exportFolder, "covariate_value_dist.csv"),
        covariateRefFileName = file.path(exportFolder, "covariate_ref.csv"),
        analysisRefFileName = file.path(exportFolder, "analysis_ref.csv"),
        timeRefFileName = file.path(exportFolder, "temporal_time_ref.csv"),
        cohortCounts = cohortCounts,
        minCellCount = minCellCount
      )
      Andromeda::close(output)
      rm("output")
    } else {
      ParallelLogger::logInfo("  - Skipping in incremental mode.")
    }
    recordTasksDone(
      cohortId = subset$cohortId,
      task = "runCohortCharacterization",
      checksum = subset$checksum,
      recordKeepingFile = recordKeepingFile,
      incremental = incremental
    )
    delta <- Sys.time() - startTemporalCohortCharacterization
    ParallelLogger::logInfo(
      " - Running Temporal Characterization took ",
      signif(delta, 3),
      " ",
      attr(delta, "units")
    )
  }

  # Writing metadata file
  ParallelLogger::logInfo("Retrieving metadata information and writing metadata")
  
  packageName <- utils::packageName()
  packageVersion <- if (!methods::getPackageName() == ".GlobalEnv") {
    as.character(utils::packageVersion(packageName))
  } else {
    ''
  }
  variableField <- c(
    "timeZone",
    #1
    "runTime",
    #2
    "runTimeUnits",
    #3
    "packageDependencySnapShotJson",
    #4
    "argumentsAtDiagnosticsInitiationJson",
    #5
    "rversion",
    #6
    "currentPackage",
    #7
    "currentPackageVersion",
    #8
    "sourceDescription",
    #9
    "cdmSourceName",
    #10
    "sourceReleaseDate",
    #11
    "cdmVersion",
    #12
    "cdmReleaseDate",
    #13
    "vocabularyVersion",
    #14
    "datasourceName",
    #15
    "datasourceDescription",
    #16
    "vocabularyVersionCdm",
    #17
    "vocabularyVersion",
    #18
    "observationPeriodMinDate",
    #19
    "observationPeriodMaxDate",
    #20
    "personsInDatasource",
    #21
    "recordsInDatasource",
    #22
    "personDaysInDatasource" #24
  )
  valueField <-   c(
    as.character(Sys.timezone()),
    #1
    as.character(as.numeric(
      x = delta, units = attr(delta, "units")
    )),
    #2
    as.character(attr(delta, "units")),
    #3
    packageDependencySnapShotJson,
    #4
    argumentsAtDiagnosticsInitiationJson,
    #5
    as.character(R.Version()$version.string),
    #6
    as.character(nullToEmpty(packageName)),
    #7
    as.character(nullToEmpty(packageVersion)),
    #8
    as.character(nullToEmpty(
      cdmSourceInformation$sourceDescription
    )),
    #9
    as.character(nullToEmpty(cdmSourceInformation$cdmSourceName)),
    #10
    as.character(nullToEmpty(
      cdmSourceInformation$sourceReleaseDate
    )),
    #11
    as.character(nullToEmpty(cdmSourceInformation$cdmVersion)),
    #12
    as.character(nullToEmpty(cdmSourceInformation$cdmReleaseDate)),
    #13
    as.character(nullToEmpty(
      cdmSourceInformation$vocabularyVersion
    )),
    #14
    as.character(databaseName),
    #15
    as.character(databaseDescription),
    #16
    as.character(nullToEmpty(cdmSourceInformation$vocabularyVersion)),
    #17
    as.character(vocabularyVersion),
    #18
    as.character(observationPeriodDateRange$observationPeriodMinDate),
    #19
    as.character(observationPeriodDateRange$observationPeriodMaxDate),
    #20
    as.character(observationPeriodDateRange$persons),
    #21
    as.character(observationPeriodDateRange$records),
    #22
    as.character(observationPeriodDateRange$personDays) #24
  )
  metadata <- dplyr::tibble(
    databaseId = as.character(!!databaseId),
    startTime = paste0("TM_", as.character(startDateTime)),
    variableField = variableField,
    valueField = valueField
  )
  writeToCsv(
    data = metadata,
    fileName = file.path(exportFolder, "metadata.csv"),
    incremental = TRUE,
    start_time = as.character(startDateTime)
  )
  # Add all to zip file----
  ParallelLogger::logInfo("Adding results to zip file")
  zipName <-
    file.path(exportFolder, paste0("Results_", databaseId, ".zip"))
  files <- list.files(exportFolder, pattern = ".*\\.csv$")
  oldWd <- setwd(exportFolder)
  on.exit(setwd(oldWd), add = TRUE)
  DatabaseConnector::createZipFile(zipFile = zipName, files = files)
  ParallelLogger::logInfo(" - Results are ready for sharing at: ", zipName)
  
  delta <- Sys.time() - startDateTime
  
  ParallelLogger::logInfo("Computing all diagnostics took ",
                          signif(delta, 3),
                          " ",
                          attr(delta, "units"))
}
